# Project Practical Machine Learning
### Predict the manner in which Users did Exercise
==============================================

```{r setoptions, echo=FALSE}
opts_chunk$set(echo=TRUE,warning=FALSE,fig.path='figure/')
#if (require(AppliedPredictiveModeling)) library(AppliedPredictiveModeling)
if (require(caret)) library(caret)
#if (require(rattle)) library(rattle)
#if (require(rpart)) library(rpart)
#if (require(rpart.plot)) library(rpart.plot)
if (require(randomForest)) library(randomForest)
```

The main objective of the project is to figure out a model that predict how users did exercice (_classe_ variable). You may use any of the other variables to predict with. In order to fullfil the project it is necessary to:
* create a report describing how you built your model, 
* describe how it is used cross validation,
* Explain what you think the expected out of sample error is, and,
* why you made the choices you did

At the end we are going to use our prediction model to predict 20 different test cases.

### 1.- Loading and preprocessing the data

First we load the csv file (both files can be found in [train](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) and [test](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv). The data from the project comes from [project](http://groupware.les.inf.puc-rio.br/har).

```{r Loading}
myd1.train<-read.csv("pml-training.csv",header=T,na.strings=c("NA",""))
myd1.test<-read.csv("pml-testing.csv",header=T,na.strings=c("NA",""))
dim.train<-dim(myd1.train)
#head(myd)
#str(myd1)
```
Then we can remove variables with no usefull information. We can account for the number of NA's, so any column with a number of NA's long enough will be discarded.

```{r DiscardingVariables}
idxToKeep <- c()
for (i in 1:dim.train[2]) {
  if(length(which(!is.na(myd1.train[,i])))==dim.train[1]) { #number of no NA's = to length of rows All data available
    idxToKeep <- c(idxToKeep,i)
  }
}
mydfTrain <- myd1.train[,idxToKeep]
mydfTest <- myd1.test[,idxToKeep]

# Variable x, user_name to num_window will be discarded as well to be non useful 
mydfTrain<-mydfTrain[,8:dim(mydfTrain)[2]]
mydfTest<-mydfTest[,8:dim(mydfTest)[2]]
#names(mydfTrain)
```

It is a good suggestion to clean up near zero variance parameters. In order to deploy this step we are going to use _nearZeroVar_ to our data frame.

```{r checkZeroVar}
nzv <- nearZeroVar(mydfTrain,saveMetrics=TRUE)
# How many variable are ZeroVar
sum(which(nzv$zeroVar==TRUE))
```

There is no need to remove further features because all variables are *OK*


## Methods

In order to check preprocessing and cross validation we prepare two sets, data split into 70 and 30 percent.

```{r CreateSets}
trainId <- createDataPartition(mydfTrain$classe, list = FALSE, p = 0.7)
trainSet = mydfTrain[trainId, ]
crosvSet = mydfTrain[-trainId, ]

# Make the preprocess model
# We need all columns numeric 
colOk = which(lapply(trainSet, class) %in% c("numeric"))
predMod <- preProcess(trainSet[,colOk], method = c("knnImpute"))

predTrainSet <- cbind(trainSet$classe, predict(predMod,trainSet[,colOk]))
predCrosvSet <- cbind(crosvSet$classe, predict(predMod, crosvSet[, colOk]))

predTestSet <- predict(predMod, mydfTest[, colOk])
names(predTrainSet)[1] <- "classe"
names(predCrosvSet)[1] <- "classe"
```

### Random Forest
Here we build a model based on Random Forest and show the accuracy over training set. Obviously over optimistic (as it is expected). For the cross validation accuracy, the results are pretty reasonable close to 99.5%.

```{r RandomForest1}
RFMod1 <- randomForest(classe ~ ., predTrainSet)
trainPred <- predict(RFMod1, predTrainSet)
print(confusionMatrix(trainPred, predTrainSet$classe))

#Cross Validation Accuracy 
crosvSetPred <- predict(RFMod1, predCrosvSet)
print(confusionMatrix(crosvSetPred, predCrosvSet$classe))
```


## Test 
Here we can check the results of the model over the test data set.


```{r Prediction Over Test}
# Our Model
print(RFMod1)

# Our Estimation
predict(RFMod1, predTestSet)
```


## Conclusions
It turns out that the model presented give a really good answer over the _test_ Other models have been tested but discarded to generate poor results (for instance a classification tree with _rpart_) 

The important issue is that before putting our model under the test, we cheched it and did the job quite well. So as a conclusion we never know if our model works fine until it is deployed with a real environment. 

